{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7b48f6",
   "metadata": {},
   "source": [
    "# 📈 مشروع ESRGAN لتحسين دقة الصور باستخدام بيانات DIV2K (محلية)\n",
    "دفتر Jupyter احترافي لتحميل البيانات، بناء النموذج، تدريبه، وتقييم الأداء باستخدام PSNR و SSIM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112a522",
   "metadata": {},
   "source": [
    "## 1. استيراد المكتبات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8951f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1caba8a",
   "metadata": {},
   "source": [
    "## 2. تحميل بيانات DIV2K من المجلد المحلي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⚠️ قم بتعديل المسارات حسب جهازك\n",
    "lr_dir = '../data/DIV2K_valid_LR_bicubic/X4'\n",
    "hr_dir = '../data/DIV2K_valid_HR'\n",
    "\n",
    "# تحويلات مختلفة للـ LR و HR\n",
    "transform_lr = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # لتتناسب مع ESRGAN input\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_hr = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # لتتناسب مع مخرجات ESRGAN\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform_lr=None, transform_hr=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.transform_lr = transform_lr\n",
    "        self.transform_hr = transform_hr\n",
    "        self.files = sorted(os.listdir(lr_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path = os.path.join(self.lr_dir, self.files[idx])\n",
    "        hr_path = os.path.join(self.hr_dir, self.files[idx].replace('x4', ''))\n",
    "        lr_img = Image.open(lr_path).convert('RGB')\n",
    "        hr_img = Image.open(hr_path).convert('RGB')\n",
    "        if self.transform_lr:\n",
    "            lr_img = self.transform_lr(lr_img)\n",
    "        if self.transform_hr:\n",
    "            hr_img = self.transform_hr(hr_img)\n",
    "        return lr_img, hr_img\n",
    "\n",
    "# تحميل البيانات\n",
    "dataset = DIV2KDataset(lr_dir, hr_dir, transform_lr=transform_lr, transform_hr=transform_hr)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc376c0d",
   "metadata": {},
   "source": [
    "## 3. بناء نموذج ESRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb820b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat([x, x1], 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat([x, x1, x2], 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat([x, x1, x2, x3], 1)))\n",
    "        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], 1))\n",
    "        return x + x5 * 0.2\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, nf, gc=32):\n",
    "        super().__init__()\n",
    "        self.rdb1 = ResidualDenseBlock(nf, gc)\n",
    "        self.rdb2 = ResidualDenseBlock(nf, gc)\n",
    "        self.rdb3 = ResidualDenseBlock(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.rdb3(self.rdb2(self.rdb1(x))) * 0.2\n",
    "\n",
    "class ESRGANGenerator(nn.Module):\n",
    "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=5, gc=32):\n",
    "        super().__init__()\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1)\n",
    "        self.rrdb_blocks = nn.Sequential(*[RRDB(nf, gc) for _ in range(nb)])\n",
    "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea = self.conv_first(x)\n",
    "        trunk = self.trunk_conv(self.rrdb_blocks(fea))\n",
    "        fea = fea + trunk\n",
    "        fea = self.lrelu(F.interpolate(self.upconv1(fea), scale_factor=2))\n",
    "        fea = self.lrelu(F.interpolate(self.upconv2(fea), scale_factor=2))\n",
    "        return self.conv_last(fea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a395aff",
   "metadata": {},
   "source": [
    "## 4. تدريب النموذج"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543c2c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/50 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m lr_imgs, hr_imgs \u001b[38;5;241m=\u001b[39m lr_imgs\u001b[38;5;241m.\u001b[39mto(device), hr_imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(lr_imgs)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:128\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3753\u001b[0m, in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3751\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3753\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39ml1_loss(\n\u001b[0;32m   3755\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3756\u001b[0m )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # لعرض شريط تقدم التدريب\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ESRGANGenerator().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "epochs = 3  # مثلاً 3 تكرارات\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for lr_imgs, hr_imgs in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "        preds = model(lr_imgs)\n",
    "        loss = loss_fn(preds, hr_imgs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"✅ نهاية Epoch {epoch+1}, متوسط الخسارة: {epoch_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a4369",
   "metadata": {},
   "source": [
    "## 5. تقييم النموذج وعرض النتائج"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "lr_img, hr_img = dataset[0]\n",
    "with torch.no_grad():\n",
    "    input_tensor = lr_img.unsqueeze(0).to(device)\n",
    "    output = model(input_tensor).squeeze().cpu().clamp(0, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(to_pil_image(lr_img))\n",
    "plt.title('Low Resolution')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(to_pil_image(output))\n",
    "plt.title('Super Resolved')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(to_pil_image(hr_img))\n",
    "plt.title('High Resolution')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sr_np = output.permute(1, 2, 0).numpy()\n",
    "hr_np = hr_img.permute(1, 2, 0).numpy()\n",
    "print(f\"PSNR: {psnr(hr_np, sr_np):.2f}\")\n",
    "print(f\"SSIM: {ssim(hr_np, sr_np, multichannel=True):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
